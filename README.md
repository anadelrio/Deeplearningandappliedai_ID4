
# VQâ€‘VAE with Geodesic Quantization

This project implements a modified VQâ€‘VAE pipeline where latentâ€“space quantization is performed aâ€‘posteriori using geodesic distances on a kâ€‘NN graph.  
The goal is to evaluate whether this strategy yields better discrete representations than the classical endâ€‘toâ€‘endâ€‘trained VQâ€‘VAE.

---

## Project Structure (current & upcoming)

```text
vqvaeâ€‘geodesic/
â”‚
â”œâ”€â”€ models/              # neuralâ€‘network architectures
â”‚â”œâ”€â”€ vae.py              # Variational Autoencoder (done)
â”‚â”œâ”€â”€ vqvae.py            # classical VQâ€‘VAE (todo)
â”‚â””â”€â”€ geodesic_vq.py      # quantizationâ€‘viaâ€‘geodesics (todo)
â”‚
â”œâ”€â”€ train/               # training & quantization scripts
â”‚â”œâ”€â”€ train_vae.py        # trains the VAE, logs metrics, saves checkpoint (done)
â”‚â”œâ”€â”€ train_vqvae.py      # baseline VQâ€‘VAE training (todo)
â”‚â””â”€â”€ train_geodesic.py   # kâ€‘NN graph + geodesic kâ€‘means (todo)
â”‚
â”œâ”€â”€ utils/               # helper functions
â”‚â”œâ”€â”€ metrics.py          # ELBO / BCE / KLD loss utilities
â”‚â”œâ”€â”€ plot.py             # loss curves & reconstruction visualisation
â”‚â”œâ”€â”€ datasets.py         # custom dataset wrappers (todo)
â”‚â””â”€â”€ graph_tools.py      # kâ€‘NN graph & geodesic distances (todo)
â”‚
â”œâ”€â”€ experiments/         # autoâ€‘generated results
â”‚â”œâ”€â”€ logs/               # CSV training logs
â”‚â”‚â””â”€â”€ vae_train_log.csv  # created by train_vae.py (done)
â”‚â””â”€â”€ checkpoints/        # model checkpoints
â”‚  â””â”€â”€ vae_mnist.pt      # trained VAE weights (done)
â”‚
â”œâ”€â”€ data/                # downloaded datasets
â”‚â””â”€â”€ MNIST/              # raw .gz files (torchvision â‰¥0.15 no longer writes processed/)
â”‚
â””â”€â”€ report/              # final PDF report (to be added)
```

-----------------------------------------------------------------------------------------------------------------------------------------------------------

## What each file does

File -> Description 

`models/vae.py` -> defines a simple fullyâ€‘connected Variational Autoencoder for 28Ã—28 MNIST digits. Takes a batch of images, encodes to Î¼ & logÏƒÂ², samples z, decodes back to pixel space. 

`train/train_vae.py` -> loads MNIST, trains the VAE using ELBO loss (via `utils.metrics.elbo_loss`), logs losses to `experiments/logs/vae_train_log.csv`, and saves the checkpoint to `experiments/checkpoints/vae_mnist.pt`. 

`utils/metrics.py` -> implements `elbo_loss`: returns total ELBO, reconstruction BCE, and KL divergence. 

`utils/plot.py` -> reads the CSV log and plots ELBO/BCE/KLD curves; loads the checkpoint and shows original vs reconstruction image grids. 

`experiments/logs/vae_train_log.csv` -> autoâ€‘generated CSV with `epoch,total_loss,bce,kld`. 

`experiments/checkpoints/vae_mnist.pt` -> serialized `state_dict` of the trained VAE. 

`data/MNIST/raw/` -> original MNIST files (`*.gz`). Torchvision â‰¥0.15 keeps them in memory.

> **Upcoming files** (`vqvae.py`, `graph_tools.py`, etc.) are placeholders for the next stages (classical VQâ€‘VAE baseline and geodesic quantization).

---

## ðŸš€ How to run

```bash
# 1) install deps
pip install -r requirements.txt

# 2) train the VAE
python train/train_vae.py --epochs 10

# 3) plot training curves & reconstructions
python utils/plot.py

# (future)
# 4) build kâ€‘NN graph & geodesic quantisation
# python train/train_geodesic.py
```

---

## ðŸ“Š Metrics logged

- **ELBO** (total loss)  
- **BCE** reconstruction loss  
- **KL Divergence**  
- *(planned)* Geodesic clustering distortion, intraâ€‘/interâ€‘cluster distances.

---

## ðŸ‘¥ Contributors

- **Ana** â€” VAE implementation, theory review, documentation.  
- **DarÃ­o** â€” Geodesic clustering, codebook quantisation (upcoming).

---

## ðŸ“š References

- Tenenbaum *etâ€¯al.*, **Isomap** (2000)  
- vanâ€¯denâ€¯Oord *etâ€¯al.*, **VQâ€‘VAE** (2017)  
- Duque *etâ€¯al.*, **Geodesic Clustering in VAEs** (2021)
