# VQ‑VAE with Geodesic Quantization
This project implements a modified VQ‑VAE pipeline where latent–space quantization is performed a‑posteriori using geodesic distances on a k‑NN graph.  
The goal is to evaluate whether this strategy yields better discrete representations than the classical end‑to‑end‑trained VQ‑VAE.
---

## Project Structure

```
.
├── .gitignore                         # Ignore __pycache__, data cache, etc.
├── README.md                          
├── data.zip                           # zipped raw MNIST
├── requirements.txt                   # Python dependencies
│
├── experiments/
│   ├──images/
│   │   └── ar_generated.jpg           #image generated by the AR                    
│   ├── checkpoints/                   # saved model weights
│   │   ├── vae_mnist.pt               # trained VAE
│   │   ├── vqvae_mnist.pt             # trained VQ-VAE
│   │   └── autoregressive_model.pt    
│   ├── latent-spaces/                 
│   │   ├── geodesic_centroids.npy     # centroids found by geodesic k-means
│   │   ├── geodesic_codes.npy         # integer code-book indices per sample
│   │   ├── geodesic_labels.npy        # cluster labels
│   │   ├── latents.pt
│   │   └── generated_codes.npy        # codes synthesised for Geo-VQ recon
│   ├──logs/
│   │   ├── vae_train_log.csv          # epoch-wise ELBO, BCE, KL
│   │   ├── vqvae_train_log.csv        # epoch-wise BCE + commit loss
│   │   └── results_table.csv          # final numbers used in the paper
│   └──reconstructions/
│       ├──comparison.jpg
│       ├──generated_from_codes.jpg
│       └──reconstruction_example.jpg
│       
├── report/    
│   ├── Report.pdf #the report of the project                       
│   ├── Appendix.pdf       #the appendix of the project
│   └── figures/                       
│       └──reconstruction_comparison.jpg  #image used in the report to compare the models
│     
├── models/                           
│   ├── vae.py                         # fully-connected VAE for 28×28 images
│   └── vqvae.py                       # VQ-VAE + VectorQuantizer class
│
├── train/                             
│   ├── train_vae.py                   # trains the baseline VAE
│   ├── train_vqvae.py                 # trains VQ-VAE (and Geo-VQ when β=0)
│   ├── train_geodesic.py              # runs geodesic k-means on VAE latents
│   ├── export_latents.py              # dumps latent vectors to *.pt
│   ├── train_autoregressive.py        # Train autoregressive GRU on discrete codes
│   ├── generate_autoregressive.py     # generate and visualize new samples via GRU
│   ├── assign_codes.py                # Assign discrete codes to dataset
│   └── evaluate_reconstructions.py    # writes metrics + saves comparison fig
│
└── utils/                        # helper functions / plotting utilities
    ├── data_utils.py                  # `get_train_loader`, `get_test_loader`
    ├── losses.py                      # `elbo_loss`, `vqvae_loss`
    ├── graph_tools.py                 # build k-NN graph, shortest paths
    ├── plot.py                        # plot per-epoch losses & recon samples
    ├── plot_clusters.py               # 2-D latent TSNE + cluster colouring
    └── plot_comparison.py             # generate reconstruction grid figure


```

---

## How to Run

```bash
# 1. Create & activate virtual environment and install dependencies
python3 -m venv .venv
source .venv/bin/activate
pip install --upgrade pip
pip install -r requirements.txt

# 2. Train the VAE
python train/train_vae.py --epochs 10 --batch_size 128 --latent_dim 20

# 3. Train the VQ-VAE
python train/train_vqvae.py --epochs 30 --batch_size 128 --latent_dim 20 --embeddings 64 --beta 0.25

# 4. Perform Geodesic Quantization
python train/export_latents.py
python train/train_geodesic.py
python train/assign_codes.py

# 5. Evaluate Reconstructions
python train/evaluate_reconstructions.py --model vae
python train/evaluate_reconstructions.py --model vqvae
python train/evaluate_reconstructions.py --model geode

# 6. Generate Qualitative Comparison Plot
python utils/plot_comparison.py \
  --vae_ckpt experiments/checkpoints/vae_mnist.pt \
  --vqvae_ckpt experiments/checkpoints/vqvae_mnist.pt \
  --geode_codes experiments/latent-spaces/geodesic_codes.npy \
  --out report/figures/reconstruction_comparison.png \
  --n 8

# 7. Train & Run Autoregressive Model
python train/train_autoregressive.py
python train/generate_autoregressive.py --n 25 --seq_len 32 --temperature 0.9
```

---

## Metrics

- **Reconstruction Loss (BCE)**  
  Pixel‐wise binary cross‐entropy summed over all test samples.

- **Regularization Term**  
  - **VAE:** KL divergence  
    \(\displaystyle \mathrm{KL}\bigl(q_\phi(z\mid x)\,\|\,p(z)\bigr)\)  
  - **VQ-VAE / Geo-VQ:** commitment loss  
    \(\displaystyle \beta\,\bigl\|\mathrm{sg}[z_e(x)] - e\bigr\|^2\)

- **Total Loss**  
  - **VAE:** ELBO = BCE + KL  
  - **VQ-VAE / Geo-VQ:** BCE + commitment

- **Sequence Negative Log-Likelihood (NLL)**  
  Bits‐per‐token of the autoregressive GRU:  
  \(\displaystyle \mathrm{NLL} = -\frac{1}{M}\sum_{t=1}^{M}\log P\bigl(c_t \mid c_{<t}\bigr)\)

---

## Note

MNIST will be downloaded automatically by the script, so you do not need to unzip the data provided.